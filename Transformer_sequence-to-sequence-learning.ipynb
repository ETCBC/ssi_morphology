{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Transformer based sequence-to-sequence learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(filename, data_dict):\n",
    "\n",
    "    with open(filename) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            bo, ch, ve, text = tuple(line.strip().split('\\t'))\n",
    "            words = text.split()\n",
    "            for w in words:  \n",
    "                # in the output data, composite placenames have a '_', which cannot be found in the input data\n",
    "                words_split = w.split('_')               \n",
    "                for word_split in words_split:\n",
    "                    data_dict[bo].append(word_split)\n",
    "        \n",
    "            line = fp.readline()\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file = '/content/gdrive/My Drive/data/t-in_voc'\n",
    "input_file = 'data/t-in_voc'\n",
    "input_data = collections.defaultdict(list)\n",
    "\n",
    "#output_file = '/content/gdrive/My Drive/data/t-out'\n",
    "output_file = 'data/t-out'\n",
    "output_data = collections.defaultdict(list)\n",
    "\n",
    "input_data = read_data_from_file(input_file, input_data)\n",
    "output_data = read_data_from_file(output_file, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reduction consists of removing the left-most marker from all\n",
    "# the doubly marked prefixes and the redundant colon of the vowel\n",
    "# pattern mark.\n",
    "\n",
    "prefixes = ['!', ']', '@']\n",
    "\n",
    "def mc_reduce(s):\n",
    "   for c in prefixes:\n",
    "      s = re.sub(f'{c}([^{c}]*{c})', r'\\1', s)\n",
    "   return s.replace(':', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_sequences(data_dict, sequence_length):\n",
    "    \n",
    "    all_sequences = []\n",
    "    for words_list in data_dict.values():\n",
    "\n",
    "        for w in range(len(words_list) - sequence_length + 1):\n",
    "    \n",
    "            seq = ' '.join([words_list[ind] for ind in list(range(w, w + sequence_length))])\n",
    "        \n",
    "            all_sequences.append(seq)\n",
    "        \n",
    "    return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_out_sequences(data_dict, sequence_length):\n",
    "    \n",
    "    all_sequences = []\n",
    "    for words_list in data_dict.values():\n",
    "\n",
    "        for w in range(len(words_list) - sequence_length + 1):\n",
    "    \n",
    "            seq = ' '.join([words_list[ind] for ind in list(range(w, w + sequence_length))])\n",
    "        \n",
    "            seq = mc_reduce(seq)\n",
    "            all_sequences.append(seq)\n",
    "        \n",
    "    return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 1\n",
    "\n",
    "all_in_seqs = make_in_sequences(input_data, sequence_length)\n",
    "all_out_seqs = make_out_sequences(output_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B.:R;>CIJT',\n",
       " 'B.@R@>',\n",
       " '>:ELOHIJM',\n",
       " '>;T',\n",
       " 'HAC.@MAJIM',\n",
       " 'W:>;T',\n",
       " 'H@>@REY',\n",
       " 'W:H@>@REY',\n",
       " 'H@J:T@H',\n",
       " 'TOHW.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_in_seqs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-R>CJT/',\n",
       " 'BR>[',\n",
       " '>LH(J(M/JM',\n",
       " '>T',\n",
       " 'H-CMJ(M/(JM',\n",
       " 'W->T',\n",
       " 'H->RY/a',\n",
       " 'W-H->RY/a',\n",
       " 'HJ(H[&TH',\n",
       " 'THW/']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_seqs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49490"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_out_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for inp, outp in zip(all_in_seqs, all_out_seqs):\n",
    "    \n",
    "    outp = \"s \" + outp + \" e\"\n",
    "    text_pairs.append((inp, outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<@NIJ', 's <NJ/ e')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D', '<', 'd', 'K', 's', '[', 'B', '(', ' ', 'c', 'F', 'M', '=', ']', 'n', 'u', '-', 'Z', '>', '!', 'V', '+', 'C', 'H', 'o', 'T', 'S', 'X', 'N', 'a', 'p', 'R', 'W', '~', 'e', 'Y', 'J', '&', '/', 'P', 'G', 'L', 'Q'}\n",
      "43\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "out_set = set()\n",
    "max_len = 0\n",
    "\n",
    "for inp, outp in train_pairs:\n",
    "    \n",
    "    if len(outp) > max_len:\n",
    "        max_len = len(outp)\n",
    "        \n",
    "    for char in outp:\n",
    "        out_set.add(char)\n",
    "    \n",
    "print(out_set)\n",
    "print(len(out_set))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D', '<', 'K', 'U', ';', 'B', 'F', 'M', '@', 'O', ':', '.', '>', 'Z', 'V', 'C', 'A', 'H', 'T', 'S', 'X', 'N', 'R', 'W', 'Y', 'J', 'I', 'P', 'G', 'L', 'E', 'Q', '*'}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "in_set = set()\n",
    "\n",
    "\n",
    "for inp, outp in train_pairs:\n",
    "        \n",
    "    for char in inp:\n",
    "        in_set.add(char)\n",
    "    \n",
    "print(in_set)\n",
    "print(len(in_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the input and output text pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "vocab_size = 46\n",
    "sequence_length = 20\n",
    "\n",
    "def char_split(input_data):\n",
    "  return tf.strings.unicode_split(input_data, 'UTF-8')\n",
    "\n",
    "source_vectorization = TextVectorization(\n",
    "    #max_tokens=vocab_size,\n",
    "    max_tokens=vocab_size,\n",
    "    split=char_split,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=None,\n",
    ")\n",
    "target_vectorization = TextVectorization(\n",
    "    #max_tokens=vocab_size,\n",
    "    split=char_split,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=None, \n",
    ")\n",
    "train_input_texts = [pair[0] for pair in train_pairs]\n",
    "train_output_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_input_texts)\n",
    "target_vectorization.adapt(train_output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210474"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing training and validation datasets for the translation task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(inp, outp):\n",
    "    inp = source_vectorization(inp)\n",
    "    outp = target_vectorization(outp)\n",
    "    return ({\n",
    "        \"input\": inp,\n",
    "        \"output\": outp[:, :-1],\n",
    "    }, outp[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    in_texts, out_texts = zip(*pairs)\n",
    "    in_texts = list(in_texts)\n",
    "    out_texts = list(out_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((in_texts, out_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['input'].shape: (64, 20)\n",
      "inputs['output'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['input'].shape: {inputs['input'].shape}\")\n",
    "    print(f\"inputs['output'].shape: {inputs['output'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### The Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**PositionalEmbedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**End-to-end Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embed_dim = 30\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = tensorflow.keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the sequence-to-sequence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3289/3289 [==============================] - 302s 91ms/step - loss: 0.2830 - accuracy: 0.8614 - val_loss: 0.1026 - val_accuracy: 0.9424\n",
      "Epoch 2/12\n",
      "3289/3289 [==============================] - 289s 88ms/step - loss: 0.1133 - accuracy: 0.9437 - val_loss: 0.0672 - val_accuracy: 0.9629\n",
      "Epoch 3/12\n",
      "3289/3289 [==============================] - 281s 85ms/step - loss: 0.0832 - accuracy: 0.9591 - val_loss: 0.0603 - val_accuracy: 0.9672\n",
      "Epoch 4/12\n",
      "3289/3289 [==============================] - 287s 87ms/step - loss: 0.0697 - accuracy: 0.9658 - val_loss: 0.0496 - val_accuracy: 0.9732\n",
      "Epoch 5/12\n",
      "3289/3289 [==============================] - 287s 87ms/step - loss: 0.0619 - accuracy: 0.9697 - val_loss: 0.0469 - val_accuracy: 0.9756\n",
      "Epoch 6/12\n",
      "3289/3289 [==============================] - 288s 88ms/step - loss: 0.0566 - accuracy: 0.9722 - val_loss: 0.0458 - val_accuracy: 0.9758\n",
      "Epoch 7/12\n",
      "3289/3289 [==============================] - 302s 92ms/step - loss: 0.0532 - accuracy: 0.9739 - val_loss: 0.0439 - val_accuracy: 0.9769\n",
      "Epoch 8/12\n",
      "3289/3289 [==============================] - 296s 90ms/step - loss: 0.0506 - accuracy: 0.9752 - val_loss: 0.0434 - val_accuracy: 0.9767\n",
      "Epoch 9/12\n",
      "3289/3289 [==============================] - 287s 87ms/step - loss: 0.0485 - accuracy: 0.9762 - val_loss: 0.0429 - val_accuracy: 0.9779\n",
      "Epoch 10/12\n",
      "3289/3289 [==============================] - 315s 96ms/step - loss: 0.0468 - accuracy: 0.9770 - val_loss: 0.0425 - val_accuracy: 0.9776\n",
      "Epoch 11/12\n",
      "3289/3289 [==============================] - 321s 97ms/step - loss: 0.0454 - accuracy: 0.9777 - val_loss: 0.0403 - val_accuracy: 0.9787\n",
      "Epoch 12/12\n",
      "3289/3289 [==============================] - 317s 96ms/step - loss: 0.0439 - accuracy: 0.9784 - val_loss: 0.0391 - val_accuracy: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12bc14ba8e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=12, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**\"Translating\" new sentences with our Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "L:HIM.OL\n",
      "s   L H H ] M L M / e\n",
      "-\n",
      "T.:HIJ\n",
      "s   T T T = e\n",
      "-\n",
      "W@KOH\n",
      "s   W W K H H H e\n",
      "-\n",
      "B.:KOWR\n",
      "s   B B K K K W e\n",
      "-\n",
      ">ET\n",
      "s   > T e\n",
      "-\n",
      "HAG.IT.IJ\n",
      "s   H G G T G ( T J e\n",
      "-\n",
      "K.IJ\n",
      "s   K K J J e\n",
      "-\n",
      "<IM.OW\n",
      "s   < M M M M + e\n",
      "-\n",
      "Y:B@>OWT\n",
      "s   Y Y Y > Y ( e\n",
      "-\n",
      "XAV.A>T\n",
      "s   X V V V V > T / e\n",
      "-\n",
      "D.@WID\n",
      "s   D D W W W = e\n",
      "-\n",
      "W.B:NIJTIJH@\n",
      "s   W B B - N ( T J T J J ( e\n",
      "-\n",
      "L;K:\n",
      "s   ! ! ! ( ! ( e\n",
      "-\n",
      "L@GW.R\n",
      "s   L L ( G G W e\n",
      "-\n",
      ">:ACER\n",
      "s   > > C C C C e\n",
      "-\n",
      "WAJ:DAB.;R\n",
      "s   W J J ! J ! H R e\n",
      "-\n",
      "WAJ.A<AF\n",
      "s   W J J J J ! e\n",
      "-\n",
      "L:B;JT\n",
      "s   L L L - B J e\n",
      "-\n",
      ">:ACER\n",
      "s   > > C C C C e\n",
      "-\n",
      ">@RW.R\n",
      "s   > > > R R W e\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "out_vocab = target_vectorization.get_vocabulary()\n",
    "out_index_lookup = dict(zip(range(len(out_vocab)), out_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"s\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = out_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"e\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_input_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_input_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part04_sequence-to-sequence-learning.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
